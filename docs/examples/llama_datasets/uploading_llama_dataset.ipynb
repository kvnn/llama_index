{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1448e661-eaab-4e88-9f37-80566567e677",
   "metadata": {},
   "source": [
    "# Contributing a LlamaDataset To LlamaHub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee479cd5-40eb-4e7d-92a8-42202bc700af",
   "metadata": {},
   "source": [
    "`LlamaDataset`'s storage is managed through a git repository. To contribute a dataset requires making a pull request to `llama_index/llama_datasets` Github (LFS) repository. \n",
    "\n",
    "To contribute a `LabelledRagDataset` (a subclass of `BaseLlamaDataset`), two sets of files are required:\n",
    "\n",
    "1. The `LabelledRagDataset` saved as json named `rag_dataset.json`\n",
    "2. Source document files used to create the `LabelledRagDataset`\n",
    "\n",
    "This brief notebook provides a quick example using the Paul Graham Essay text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4639b268-e0d4-40de-af97-198be1c62c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acea4a6-30a5-45fb-a5e3-f4c0ba013154",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0784e81f-7845-4d34-a196-9f699356c999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-28 14:57:09--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75042 (73K) [text/plain]\n",
      "Saving to: ‘data/paul_graham/paul_graham_essay.txt’\n",
      "\n",
      "data/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2023-11-28 14:57:09 (3.23 MB/s) - ‘data/paul_graham/paul_graham_essay.txt’ saved [75042/75042]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p 'data/paul_graham/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef0150c-1bce-4474-8fec-8b769aff192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# Load documents and build index\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"data/paul_graham/paul_graham_essay.txt\"]\n",
    ").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de410fc-8ceb-49f2-9b43-06caf3dced31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483cc49cb64247ba8f7361c7b610c717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 22/22 [00:13<00:00,  1.67it/s]\n",
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:16<00:00,  8.42s/it]\n",
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:12<00:00,  6.17s/it]\n",
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:28<00:00, 14.04s/it]\n",
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:13<00:00,  6.72s/it]\n",
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:23<00:00, 11.78s/it]\n",
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.54s/it]\n",
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.39s/it]\n",
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:20<00:00, 10.15s/it]\n",
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:13<00:00,  6.71s/it]\n",
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:23<00:00, 11.59s/it]\n",
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:31<00:00, 15.84s/it]\n",
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.39s/it]\n",
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:20<00:00, 10.27s/it]\n",
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:18<00:00,  9.15s/it]\n",
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:26<00:00, 13.17s/it]\n",
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:17<00:00,  8.85s/it]\n",
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.14s/it]\n",
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:32<00:00, 16.42s/it]\n",
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.72s/it]\n",
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:22<00:00, 11.28s/it]\n",
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:15<00:00,  7.83s/it]\n",
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:12<00:00,  6.47s/it]\n"
     ]
    }
   ],
   "source": [
    "# generate questions against chunks\n",
    "from llama_index.llama_dataset.generator import RagDatasetGenerator\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "# set context for llm provider\n",
    "gpt_35_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-4\", temperature=0.3)\n",
    ")\n",
    "\n",
    "# instantiate a DatasetGenerator\n",
    "dataset_generator = RagDatasetGenerator.from_documents(\n",
    "    documents,\n",
    "    service_context=gpt_35_context,\n",
    "    num_questions_per_chunk=2,  # set the number of questions per nodes\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "rag_dataset = dataset_generator.generate_dataset_from_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a36b15-19c5-4a69-997b-5ded125544e5",
   "metadata": {},
   "source": [
    "Now that we have our `LabelledRagDataset` generated (btw, it's totally fine to manually create one with human generated queries and reference answers!), we store this into the necessary json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34c2d0d-caf6-4960-bb37-6abd57b1ba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_dataset.save_json(\"rag_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093e63cc-f244-42ec-bbc4-b67f104a2a51",
   "metadata": {},
   "source": [
    "With the `rag_dataset.json` and source file `paul_graham_essay.txt` (note in this case, there is only one source document, but there can be several), we can perform the two steps for contributing a `LlamaDataset` into `LlamaHub`:\n",
    "\n",
    "1. Similar, to how contributions are made for `loader`'s, `agent`'s and `pack`'s, create a pull-request for `llama_hub` repository that adds a new folder for new `LlamaDataset`. This step uploads the information about the new `LlamaDataset` so that it can be presented in the `LlamaHub` UI.\n",
    "\n",
    "2. Create a pull request into `llama_datasets` repository to actually upload the data files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539c8f2f-bf6f-4a6d-b53f-0c8ca85fc891",
   "metadata": {},
   "source": [
    "### Step 0 (Pre-requisites)\n",
    "\n",
    "Fork and then clone (onto your local machine) both, the `llama_hub` Github repository as well as the `llama_datasets` one. You'll be submitting a pull requests into both of these repos from a new branch off of your forked versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611a8a75-1aaf-479a-94d0-a9127f08fe12",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "Create a new folder in `llama_datasets/` of the `llama_hub` Github repository. For example, in this case we would create a new folder `llama_datasets/paul_graham_essay_truncated`.\n",
    "\n",
    "In that folder, two files are required:\n",
    "- `card.json`\n",
    "- `README.md`\n",
    "The suggestion here is to look at previously submitted `LlamaDataset`'s and modify as needed for your new one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee08d0e-2601-4978-bdb2-e28375454d7f",
   "metadata": {},
   "source": [
    "In our current example, we need the `card.json` to look as follows\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"name\": \"Paul Graham Essay Truncated\",\n",
    "    \"description\": \"This is truncated version of the original Paul Graham Essay text file.\",\n",
    "    \"numberObservations\": 4,\n",
    "    \"containsExamplesByHumans\": false,\n",
    "    \"containsExamplesByAI\": true,\n",
    "    \"sourceUrls\": [\n",
    "        \"http://www.paulgraham.com/articles.html\"\n",
    "    ],\n",
    "    \"evaluation\": {\n",
    "        \"context_similarity\": 0.95\n",
    "        \"correctness\": 4.5,\n",
    "        \"faithfulness\": 1.0,\n",
    "        \"relevancy\": 1.0\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c332954-560a-4769-be57-787089add4d5",
   "metadata": {},
   "source": [
    "And for the `README.md`, these are pretty standard, requiring you to change the name of the dataset argument in the `download_llama_dataset()` function call.\n",
    "\n",
    "```python\n",
    "from llama_index.llama_datasets import download_llama_datasets\n",
    "from llama_index.llama_pack import download_llama_pack\n",
    "\n",
    "# download and install dependencies for rag evaluator pack\n",
    "RagEvaluatorPack = download_llama_pack(\n",
    "  \"RagEvaluatorPack\", \"./rag_evaluator_pack\"\n",
    ")\n",
    "rag_evaluator_pack = RagEvaluatorPack()\n",
    "\n",
    "# download and install dependencies for benchmark dataset\n",
    "paul_graham_qa_data = download_llama_datasets(\n",
    "  \"PaulGrahamEssayTruncatedDataset\", \"./data\"\n",
    ")\n",
    "\n",
    "# evaluate\n",
    "query_engine = VectorStoreIndex.as_query_engine()  # previously defined, not shown here\n",
    "rag_evaluate_pack.run(dataset=paul_graham_qa_data, query_engine=query_engine)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086f63ab-06f7-4a52-8271-9031530123e6",
   "metadata": {},
   "source": [
    "Finally, the last item for Step 1 is to create an entry to `llama_datasets/library.json` file. In this case:\n",
    "\n",
    "```json\n",
    "    ...,\n",
    "    \"PaulGrahamEssayTruncatedDataset\": {\n",
    "    \"id\": \"llama_datasets/paul_graham_essay_truncated\",\n",
    "    \"author\": \"andrei-fajardo\",\n",
    "    \"keywords\": [\"rag\"],\n",
    "    \"extra_files\": [\"paul_graham_essay_truncated.txt\"]\n",
    "  }\n",
    "```\n",
    "\n",
    "Note: the `extra_files` field is reserved for the source files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec3d5a7-6ba8-40fa-bed6-a95d6c8813d6",
   "metadata": {},
   "source": [
    "### Step 2 Uploading The Actual Datasets\n",
    "\n",
    "In this step, since we use Github LFS on our `llama_datasets` repo, making a contribution is exactly the same way you would make a contribution with any of our other open Github repos. That is, submit a pull request.\n",
    "\n",
    "Make a fork of the `llama_datasets` repo, and create a new folder in the `llama_datasets/` directory that matches the `id` field of the entry made in the `library.json` file. So, for this example, we'll create a new folder `llama_datasets/paul_graham_essay_truncated/`. It is here where we will add the documents and make the pull request.\n",
    "\n",
    "To this folder, add `rag_dataset.json` (it must be called this), as well as the rest of the source documents, which in our case is the `paul_graham_essay_truncated.txt` file.\n",
    "\n",
    "```sh\n",
    "llama_datasets/paul_graham_essay_truncated/\n",
    "├── paul_graham_essay_truncated.txt\n",
    "└── rag_dataset.json\n",
    "```\n",
    "\n",
    "Now, simply `git add`, `git commit` and `git push` your branch, and make your PR."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_3.10",
   "language": "python",
   "name": "llama_index_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
